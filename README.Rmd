---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
```

# CMA

<!-- badges: start -->

[![R-CMD-check](https://github.com/Reckziegel/CMA/workflows/R-CMD-check/badge.svg)](https://github.com/Reckziegel/CMA/actions) [![Codecov test coverage](https://codecov.io/gh/Reckziegel/CMA/branch/main/graph/badge.svg)](https://codecov.io/gh/Reckziegel/CMA?branch=main) [![Lifecycle: experimental](https://img.shields.io/badge/lifecycle-experimental-orange.svg)](https://lifecycle.r-lib.org/articles/stages.html#experimental)

<!-- badges: end -->

> Multivariate Distribution = Marginals + Copulas

The Copula Marginal Algorithm (CMA) is a simple two step recipe to manipulate multivariate distributions under [Fully Flexible Probabilities](https://github.com/Reckziegel/FFP).

CMA can quickly decompose any multivariate distribution between unique (*marginals*) and their shared components (*copulas*). This approach can add a high level of flexibility for estimation and simulation purposes.

# Application 1: Panic Copulas

# Application 2: "What if" Analysis

Suppose we want to know what performances could be consistent with a certain investment strategy. One could be tempted to bootstrap this series, but if this route is taken, the dependency structure would change too, which is not the ideal, since we full control over this environment (our"market").

One interesting way of solve this problem is via the Copula Marginal Algorithm (CMA).

First, separate the marginals from the copulas. Second, generate a large number of potential scenarios from a target distribution; and, finally, "glue" these outputs back via the CMA combination step.

```{r, message=FALSE, warning=FALSE}
# load packages
library(cma)
library(tidyverse)

# compute log returns
x <- matrix(diff(log(EuStockMarkets)), ncol = 4)

# First CMA Step
step_one <- cma_separation(x)
step_one
```

Once the decomposition done, the student-t distribution is fitted to the data (it could be *any* distribution):

```{r}
dist_t <- fit_t(step_one$marginal)
dist_t
```

The next step requires some `tidyverse` skills. Start building the new scenarios:

```{r}
simul_tbl <- tibble(simulations = 1:100) |> 
    mutate(new_scenarios = map(
        .x = rep(1859, 100), 
        .f = ~ generate_margins(model = dist_t, n = .x)
        )
    )
simul_tbl
```

The output of `new_scenarios()` is a list. It's easy to pass over it with the command bellow:

```{r}
simul_tbl <- simul_tbl |> 
  mutate(new_scenarios = map(new_scenarios, 1))
simul_tbl
```

Now we have a `tibble` with 100 simulations consistent with the student-t distribution fitted in the object `dist_t`.

```{r}
simul_tbl <- simul_tbl |> 
  mutate(cma_comb = map(
    .x = new_scenarios, 
    .f = ~ cma_combination(
      x      = .x,
      cdf    = step_one$cdf, 
      copula = step_one$copula
    ) 
  ), 
  weights = list(rep(0.25, 4))
  )
simul_tbl
```

```{r}
simul_tbl <- simul_tbl |> 
  mutate(pnl = map2(.x = cma_comb, 
                    .y = weights, 
                    .f = ~ as.matrix(.x) %*% .y)) |> 
  select(-c(cma_comb, weights))
simul_tbl
```

```{r out.height="80%", out.width="80%", fig.align='center'}
simul_tbl |> 
    mutate(pnl_prices = map(.x = pnl, .f = ~ cumprod(exp(.x)))) |> 
    unnest(cols = pnl_prices) |> 
  
    group_by(simulations) |> 
    mutate(rowid = 1:1859) |> 
    ungroup() |> 
  
    ggplot(aes(x = rowid, y = pnl_prices, group = simulations)) + 
    geom_line(col = "grey") + 
    scale_y_log10()
```
